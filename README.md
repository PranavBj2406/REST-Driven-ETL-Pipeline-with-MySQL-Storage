# ğŸ“ˆ Stock Data ETL Pipeline

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![MySQL](https://img.shields.io/badge/mysql-%2300f.svg?style=for-the-badge&logo=mysql&logoColor=white)

An automated ETL pipeline that fetches stock data from RapidAPI and loads it into MySQL using Apache Airflow.

## ğŸš€ What it does

1. **Extract** - Fetches real-time stock data from RapidAPI
2. **Transform** - Cleans and processes the data using Python/Pandas  
3. **Load** - Stores the processed data in MySQL database

All automated and scheduled using Apache Airflow!

https://github.com/user-attachments/assets/8aa54282-0e80-4999-9bd5-c48d1c188bdd



## ğŸ› ï¸ Tech Stack

- **Apache Airflow** - Orchestrates the entire pipeline and handles scheduling
- **Python** - Core language with pandas for data processing
- **RapidAPI** - Source for real-time stock market data
- **MySQL** - Database for storing processed stock data
- **Docker** - Containerization for easy deployment

## ğŸ“‹ Prerequisites

- Docker installed on your machine
- RapidAPI account (for stock data API)
- MySQL running locally

## ğŸ”§ Setup

1. **Clone the repo**
```bash
git clone <your-repo-url>
cd stock-etl-pipeline
```

2. **Create environment file**
Create a `.env` file with your credentials:
```env
RAPIDAPI_KEY=your_api_key_here
MYSQL_PASSWORD=your_mysql_password
```

3. **Start the pipeline**
```bash
docker-compose up -d
```

4. **Access Airflow**
- Go to `http://localhost:8080`
- Login: admin/admin
- Enable your DAG

## ğŸ“ Project Structure

```
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ my_dag.py              # Main ETL pipeline
â”œâ”€â”€ docker-compose.yml         # Docker configuration  
â”œâ”€â”€ .gitignore                 # Git ignore file
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ The Pipeline

**Extract Task** â†’ **Transform Task** â†’ **Load Task**

Each task runs in sequence, and if one fails, you can see the logs in Airflow UI to debug.

## ğŸ“Š Sample Output

The pipeline creates a `stock_prices` table in MySQL with columns like:
- symbol (e.g., AAPL, GOOGL)
- price 
- volume
- timestamp

## ğŸ› Troubleshooting

**Can't connect to MySQL?**
- Make sure MySQL is running
- Use `host.docker.internal` instead of `localhost` in your connection string

**Task failing?**
- Check the logs in Airflow UI
- Click on the failed task â†’ View Logs

## ğŸ“ Notes

- The pipeline runs automatically based on your schedule
- All logs are available in Airflow web interface
- Data gets appended to MySQL table on each run

---

**ğŸ”’ Security Note:** Never commit your `.env` file with real API keys to GitHub! The `.gitignore` file already excludes it.
